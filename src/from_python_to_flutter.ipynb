{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To convert your Python and Streamlit project into a Flutter application with real-time gesture detection, you would typically split it into two components:\n",
        "\n",
        "FastAPI Backend: Handles the model inference and real-time communication between the client (Flutter app) and the model.\n",
        "\n",
        "Flutter Frontend: Captures video input and displays the detected gesture, while also handling real-time audio playback and showing the result.\n",
        "\n",
        "Here’s a basic outline of how you can achieve this:\n",
        "\n",
        "1. FastAPI Backend for Gesture Detection\n",
        "We'll set up a FastAPI backend to handle real-time video frame processing. The backend will use the model to predict gestures and return the predictions to the Flutter frontend."
      ],
      "metadata": {
        "id": "JMJxRcNZ3uL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastAPI Backend Code:"
      ],
      "metadata": {
        "id": "5wuHJMep34ek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVe2ZMM73tef"
      },
      "outputs": [],
      "source": [
        "import fastapi\n",
        "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import io\n",
        "from pydantic import BaseModel\n",
        "import asyncio\n",
        "import threading\n",
        "import pygame\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model(\"sign_language_model.h5\")\n",
        "\n",
        "# Initialize MediaPipe\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "# Define class names\n",
        "sign_language_classes = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"عذرا\", \"طعام\", \"مرحبا\", \"مساعده\", \"منزل\", \"انا\", \"احبك\", \"لا\", \"شكرا\", \"نعم\"]\n",
        "\n",
        "# Function to process landmarks\n",
        "def process_landmarks(hand_landmarks):\n",
        "    landmarks = []\n",
        "    for lm in hand_landmarks.landmark:\n",
        "        landmarks.extend([lm.x, lm.y, lm.z])\n",
        "    return landmarks\n",
        "\n",
        "# Function to classify gestures\n",
        "def classify_gesture(frame):\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = hands.process(image_rgb)\n",
        "\n",
        "    if result.multi_hand_landmarks:\n",
        "        combined_landmarks = []\n",
        "\n",
        "        # Process first hand\n",
        "        combined_landmarks.extend(process_landmarks(result.multi_hand_landmarks[0]))\n",
        "\n",
        "        # Process second hand or pad\n",
        "        if len(result.multi_hand_landmarks) > 1:\n",
        "            combined_landmarks.extend(process_landmarks(result.multi_hand_landmarks[1]))\n",
        "\n",
        "        # Make prediction\n",
        "        landmarks_array = np.array(combined_landmarks).reshape(1, -1)\n",
        "        prediction = model.predict(landmarks_array, verbose=0)\n",
        "        class_id = np.argmax(prediction[0])\n",
        "        confidence = prediction[0][class_id]\n",
        "\n",
        "        return sign_language_classes[class_id], confidence\n",
        "\n",
        "    return None, None\n",
        "\n",
        "# FastAPI WebSocket endpoint for real-time communication\n",
        "@app.websocket(\"/ws\")\n",
        "async def websocket_endpoint(websocket: WebSocket):\n",
        "    await websocket.accept()\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            # Receive frame from Flutter app\n",
        "            frame_data = await websocket.receive_bytes()\n",
        "            nparr = np.frombuffer(frame_data, np.uint8)\n",
        "            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "            # Classify gesture\n",
        "            gesture, confidence = classify_gesture(frame)\n",
        "\n",
        "            if gesture:\n",
        "                await websocket.send_text(f\"Gesture: {gesture}, Confidence: {confidence:.2%}\")\n",
        "            else:\n",
        "                await websocket.send_text(\"No gesture detected\")\n",
        "\n",
        "    except WebSocketDisconnect:\n",
        "        print(\"Client disconnected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This FastAPI server will listen for WebSocket connections and process incoming frames. It will send back the detected gesture and confidence score to the connected client."
      ],
      "metadata": {
        "id": "PrH-WZ4y37p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Flutter Frontend\n",
        "The Flutter frontend will capture the video feed, send it to the FastAPI backend via WebSocket, and display the gesture detection result in real-time.\n",
        "\n",
        "To achieve this:\n",
        "\n",
        "Add WebSocket support in Flutter: You'll need to use the web_socket_channel Flutter package to communicate with the FastAPI backend over WebSocket.\n",
        "\n",
        "Capture video in Flutter: You can use the camera package to capture video frames in Flutter.\n",
        "\n",
        "Send frames to FastAPI: Once you have the video frames, you send them to the FastAPI server and receive the gesture detection result.\n",
        "\n"
      ],
      "metadata": {
        "id": "wjx-wYG74Apt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Flutter Code:**\n",
        "# **Add dependencies in pubspec.yaml:**"
      ],
      "metadata": {
        "id": "LFJjts7p4HUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dependencies:\n",
        "  flutter:\n",
        "    sdk: flutter\n",
        "  web_socket_channel: ^2.1.0\n",
        "  camera: ^0.9.4+5\n",
        "  audioplayers: ^0.20.1\n"
      ],
      "metadata": {
        "id": "qFgwp4DL4M9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Flutter App Code:**"
      ],
      "metadata": {
        "id": "8QUFUqtt4STx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import 'package:flutter/material.dart';\n",
        "import 'package:web_socket_channel/web_socket_channel.dart';\n",
        "import 'package:camera/camera.dart';\n",
        "import 'dart:typed_data';\n",
        "import 'dart:async';\n",
        "import 'dart:convert';\n",
        "\n",
        "void main() {\n",
        "  runApp(MyApp());\n",
        "}\n",
        "\n",
        "class MyApp extends StatelessWidget {\n",
        "  @override\n",
        "  Widget build(BuildContext context) {\n",
        "    return MaterialApp(\n",
        "      home: SignLanguageDetection(),\n",
        "    );\n",
        "  }\n",
        "}\n",
        "\n",
        "class SignLanguageDetection extends StatefulWidget {\n",
        "  @override\n",
        "  _SignLanguageDetectionState createState() => _SignLanguageDetectionState();\n",
        "}\n",
        "\n",
        "class _SignLanguageDetectionState extends State<SignLanguageDetection> {\n",
        "  CameraController? _cameraController;\n",
        "  WebSocketChannel? _channel;\n",
        "  bool _isCameraInitialized = false;\n",
        "\n",
        "  @override\n",
        "  void initState() {\n",
        "    super.initState();\n",
        "    _initializeCamera();\n",
        "    _connectWebSocket();\n",
        "  }\n",
        "\n",
        "  Future<void> _initializeCamera() async {\n",
        "    final cameras = await availableCameras();\n",
        "    final firstCamera = cameras.first;\n",
        "\n",
        "    _cameraController = CameraController(firstCamera, ResolutionPreset.medium);\n",
        "    await _cameraController?.initialize();\n",
        "\n",
        "    setState(() {\n",
        "      _isCameraInitialized = true;\n",
        "    });\n",
        "  }\n",
        "\n",
        "  void _connectWebSocket() {\n",
        "    _channel = WebSocketChannel.connect(Uri.parse(\"ws://127.0.0.1:8000/ws\"));\n",
        "    _channel?.stream.listen((data) {\n",
        "      // Handle the message from server\n",
        "      print(data);\n",
        "    });\n",
        "  }\n",
        "\n",
        "  @override\n",
        "  void dispose() {\n",
        "    _cameraController?.dispose();\n",
        "    _channel?.sink.close();\n",
        "    super.dispose();\n",
        "  }\n",
        "\n",
        "  Future<void> _sendFrameToServer(CameraImage image) async {\n",
        "    // Convert the frame to a byte array (JPEG/PNG)\n",
        "    final bytes = await _convertYUV420toJPEG(image);\n",
        "    _channel?.sink.add(bytes);\n",
        "  }\n",
        "\n",
        "  Future<Uint8List> _convertYUV420toJPEG(CameraImage image) async {\n",
        "    // Convert YUV420 frame to JPEG (You can use the `image` library for this)\n",
        "    return Uint8List(0); // Replace with actual conversion\n",
        "  }\n",
        "\n",
        "  @override\n",
        "  Widget build(BuildContext context) {\n",
        "    return Scaffold(\n",
        "      appBar: AppBar(title: Text(\"Sign Language Detection\")),\n",
        "      body: Center(\n",
        "        child: Column(\n",
        "          mainAxisAlignment: MainAxisAlignment.center,\n",
        "          children: [\n",
        "            _isCameraInitialized\n",
        "                ? CameraPreview(_cameraController!)\n",
        "                : CircularProgressIndicator(),\n",
        "            SizedBox(height: 20),\n",
        "            ElevatedButton(\n",
        "              onPressed: () {\n",
        "                // Start sending frames\n",
        "                _cameraController?.startImageStream((image) {\n",
        "                  _sendFrameToServer(image);\n",
        "                });\n",
        "              },\n",
        "              child: Text(\"Start Detection\"),\n",
        "            ),\n",
        "          ],\n",
        "        ),\n",
        "      ),\n",
        "    );\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "2AvYh32e4VPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Points:\n",
        "FastAPI WebSocket: The backend listens for WebSocket connections and processes the frames for gesture detection.\n",
        "\n",
        "Flutter Camera: The Flutter app captures video frames and sends them to the backend via WebSocket.\n",
        "\n",
        "WebSocket Communication: The app receives the detected gesture and confidence from the backend and displays it on the screen.\n",
        "\n",
        "Audio Playback:\n",
        "For audio playback, you can use Flutter’s audioplayers package or any other package that fits your needs to play the correct audio for the detected gestures.\n",
        "\n",
        "Let me know if you need further assistance on specific sections or integration!"
      ],
      "metadata": {
        "id": "NIjIjUrd4aIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Store Audio Files:**"
      ],
      "metadata": {
        "id": "d8OfRiwo4rjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure the audio files (like مرحبا.mp3, نعم.mp3, etc.) are available in your Flutter assets folder. In your pubspec.yaml, add the following:"
      ],
      "metadata": {
        "id": "a1ow5QQM4xzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flutter:\n",
        "  assets:\n",
        "    - assets/audio/\n"
      ],
      "metadata": {
        "id": "JZVgIWr24ujH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "The audioplayers package is used for playing the audio corresponding to the detected gesture.\n",
        "\n",
        "The WebSocket connection sends gesture data, and when a gesture is detected, the correct audio is played."
      ],
      "metadata": {
        "id": "uB36D3HO446u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it works:\n",
        "Real-time Gesture Detection:\n",
        "\n",
        "The Flutter app captures video from the camera using the camera package.\n",
        "\n",
        "Each frame is sent to the FastAPI backend via WebSocket.\n",
        "\n",
        "The backend processes the frame, detects the sign language gesture, and sends the gesture name back to the Flutter app.\n",
        "\n",
        "Audio Playback:\n",
        "\n",
        "Once the Flutter app receives the gesture name from the backend, it will trigger the corresponding audio file to be played using the audioplayers package.\n",
        "\n",
        "Steps to Run:\n",
        "Set up FastAPI:\n",
        "\n",
        "Ensure that the FastAPI backend is running and listening for WebSocket connections.\n",
        "\n",
        "The backend will process incoming frames and return detected gestures to the Flutter app.\n",
        "\n",
        "Set up Flutter:\n",
        "\n",
        "Ensure the audioplayers package is included and the audio files for each gesture are placed in the assets/audio/ folder.\n",
        "\n",
        "The Flutter app will capture video, send frames to the backend, and play the corresponding audio when a gesture is detected."
      ],
      "metadata": {
        "id": "jLKFbHz-4_AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run the Application:**\n",
        "\n",
        "# **Run the FastAPI server:**"
      ],
      "metadata": {
        "id": "T5eMx0no5A3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn main:app --reload\n"
      ],
      "metadata": {
        "id": "QkGyR9St5E45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run the Flutter app:**"
      ],
      "metadata": {
        "id": "aRA71Rwr5God"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flutter run\n"
      ],
      "metadata": {
        "id": "uxkUiCIw5JmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once everything is running:\n",
        "\n",
        "In real-time, as you make gestures in front of the camera, the system will detect them, and the corresponding audio for that gesture will play.\n",
        "\n",
        "Things to Check:\n",
        "Audio Files: Ensure that the audio files for each gesture (e.g., مرحبا.mp3, نعم.mp3, etc.) are available in the assets/audio/ directory and are listed in the pubspec.yaml.\n",
        "\n",
        "WebSocket Connection: Make sure the Flutter app is connected to the FastAPI WebSocket server. The WebSocket URI in the code is set to ws://127.0.0.1:8000/ws, so it assumes the backend is running locally. If you're running it on a remote server, you need to adjust the WebSocket URI accordingly."
      ],
      "metadata": {
        "id": "HAayQZsy5LFs"
      }
    }
  ]
}